# Machine Learning Algorithms Collection

This repository is a comprehensive collection of **20 fundamental and advanced machine learning algorithms**, categorized by their functional modules.

Each project within this repository serves as a deep dive into an algorithm's mechanics, dataset application, and performance metrics.

## Repository Structure

The algorithms are organized into five core pillars of Machine Learning:

| Module                                                     | Count | Description                                                              |
| :--------------------------------------------------------- | :---- | :----------------------------------------------------------------------- |
| **Regression**                             | 6     | Predicting continuous values |
| **Classification**                     | 7     | Categorizing data into discrete classes |
| **Clustering**                             | 2     | Identifying patterns in unlabeled data |
| **Reinforcement Learning**   | 2     | Optimizing decision policies based on rewards |
| **Dimensionality Reduction** | 3     | Feature extraction and class separability optimization |

---

## Algorithm Index

### Regression

- **Simple Linear Regression**: Predicting dependent variables using a single feature.
- **Multiple Linear Regression**: Linear mapping with multiple predictors.
- **Polynomial Regression**: Modeling non-linear relationships.
- **Support Vector Regression (SVR)**: Robust regression within a margin of error.
- **Decision Tree Regression**: Non-linear tree-based regression.
- **Random Forest Regression**: Ensemble learning for precise predictions.

### Classification

- **Logistic Regression**: Predicting class probabilities.
- **K-Nearest Neighbors (K-NN)**: Instance-based learning for classification.
- **Support Vector Machine (SVM)**: Maximizing margins for class separation.
- **Kernel SVM**: Handling non-linear class boundaries.
- **Naive Bayes**: Probabilistic classification based on Bayes' theorem.
- **Decision Tree Classification**: Intuitive hierarchical decision structures.
- **Random Forest Classification**: High-accuracy ensemble classification.

### Clustering

- **K-Means Clustering**: Centroid-based data partitioning.
- **Hierarchical Clustering**: Building tree-like structures for cluster identification.

### Reinforcement Learning

- **Upper Confidence Bound (UCB)**: Deterministic exploration vs. exploitation for multi-armed bandits.
- **Thompson Sampling**: Probabilistic policy optimization for maximizing CTR.

### Dimensionality Reduction

- **Principal Component Analysis (PCA)**: Unsupervised feature extraction maximizing variance.
- **Linear Discriminant Analysis (LDA)**: Supervised reduction maximizing class separability.
- **Kernel PCA**: Non-linear feature mapping.

---

## Getting Started

To explore a specific algorithm, navigate to its category folder and read the `README.md` provided in each sub-directory.

All implementations are backed by Jupyter Notebooks for interactive exploration.
